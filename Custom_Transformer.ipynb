{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import dataloader, dataset, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "#My custom Positional Encodings\n",
        "class Positional_Encoding(nn.Module):  # Fixed typo: \"Postional\" â†’ \"Positional\"\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                            (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class Masked_Attention(nn.Module):\n",
        "    \"\"\"Improved masked self-attention with efficient causal masking\"\"\"\n",
        "    def __init__(self, num_heads, embedings, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert embedings % num_heads == 0, \"embedings must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.embedings = embedings\n",
        "        self.heads = embedings // num_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # QKV projections (keeping your original names)\n",
        "        self.fc1 = nn.Linear(embedings, embedings)\n",
        "        self.fc2 = nn.Linear(embedings, embedings)\n",
        "        self.fc3 = nn.Linear(embedings, embedings)\n",
        "        self.outlayer = nn.Linear(embedings, embedings)\n",
        "\n",
        "        # Register causal mask as buffer (moved to device automatically)\n",
        "        self.register_buffer('causal_mask', None)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, embed = x.size()\n",
        "\n",
        "        # Project to Q, K, V (keeping your original variable names)\n",
        "        Q = self.fc1(x)\n",
        "        K = self.fc2(x)\n",
        "        V = self.fc3(x)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.heads).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.heads).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.heads).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        first_term = torch.matmul(Q, K.transpose(-2, -1))\n",
        "        second_term = math.sqrt(self.heads)\n",
        "        scores = first_term / second_term\n",
        "\n",
        "        # Apply padding mask if provided\n",
        "        if mask is not None:\n",
        "            padding_mask = mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, S]\n",
        "            scores = scores.masked_fill(padding_mask, float('-inf'))\n",
        "\n",
        "        # Apply causal mask (prevents attending to future tokens)\n",
        "        # FIXED: Create mask once and reuse, don't recreate every forward pass\n",
        "        if self.causal_mask is None or self.causal_mask.size(0) != seq_len:\n",
        "            # Create upper triangular matrix (1s above diagonal)\n",
        "            self.causal_mask = torch.triu(\n",
        "                torch.ones(seq_len, seq_len, device=x.device),\n",
        "                diagonal=1\n",
        "            ).bool()\n",
        "\n",
        "        scores = scores.masked_fill(self.causal_mask, float('-inf'))\n",
        "\n",
        "        # Softmax and dropout\n",
        "        val = F.softmax(scores, dim=-1)\n",
        "        val = self.dropout(val)\n",
        "\n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(val, V)\n",
        "\n",
        "        # Reshape back to [B, S, E]\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed)\n",
        "\n",
        "        # Final linear projection\n",
        "        out = self.outlayer(output)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Multihead(nn.Module):\n",
        "    \"\"\"Improved multi-head attention for encoder and cross-attention\"\"\"\n",
        "    def __init__(self, num_heads, embeding, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert embeding % num_heads == 0, \"embeding must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.embeding = embeding\n",
        "        self.head = embeding // num_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # QKV projections (keeping your original names)\n",
        "        self.fc1 = nn.Linear(embeding, embeding)\n",
        "        self.fc2 = nn.Linear(embeding, embeding)\n",
        "        self.fc3 = nn.Linear(embeding, embeding)\n",
        "        self.out_layer = nn.Linear(embeding, embeding)\n",
        "\n",
        "    def forward(self, x, enc_output=None, mask=None):\n",
        "        batch_size, seq_len, embed = x.size()\n",
        "\n",
        "        # Query from decoder (keeping your original variable names)\n",
        "        Q = self.fc1(x)\n",
        "\n",
        "        # Key and Value from encoder (if cross-attention) or from x (self-attention)\n",
        "        if enc_output is not None:\n",
        "            K = self.fc2(enc_output)\n",
        "            V = self.fc3(enc_output)\n",
        "            seq_len_kv = enc_output.size(1)\n",
        "        else:\n",
        "            K = self.fc2(x)\n",
        "            V = self.fc3(x)\n",
        "            seq_len_kv = seq_len\n",
        "\n",
        "        # Reshape for multi-head attention (split heads)\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len_kv, self.num_heads, self.head).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len_kv, self.num_heads, self.head).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head)\n",
        "\n",
        "        # Apply padding mask if provided\n",
        "        if mask is not None:\n",
        "            padding_mask = mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, S]\n",
        "            scores = scores.masked_fill(padding_mask, float('-inf'))\n",
        "\n",
        "        # Softmax and dropout\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attn, V)\n",
        "\n",
        "        # Reshape back to [B, S, E] (combine heads)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, embed)\n",
        "\n",
        "        # Final linear projection\n",
        "        return self.out_layer(out)\n",
        "\n",
        "# My Custom Postional Feed Forward Network\n",
        "class Position_Feedforward(nn.Module):\n",
        "    def __init__(self, input_ch, output_ch, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_ch, output_ch)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(output_ch, input_ch)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "         x= self.fc1(x)\n",
        "         x = self.relu(x)\n",
        "         x = self.dropout(x)\n",
        "         x = self.fc2(x)\n",
        "\n",
        "         return x"
      ],
      "metadata": {
        "id": "JsYZs5lJ2p48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import dataset, TensorDataset, dataloader\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "#Desigining Multi-layered Encoder\n",
        "class Multi_Encoder(nn.Module):\n",
        "    def __init__(self, d_model, heads, hidden_lay, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        #Encoder Stuff\n",
        "        self.E_Multi = Multihead(heads, d_model, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.E_FeedFor = Position_Feedforward(d_model, hidden_lay)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, src_mask = None):\n",
        "        # Doing Encoder Stuff\n",
        "        x1 = self.E_Multi(x, mask=src_mask)\n",
        "        x = self.norm1(x + self.dropout(x1))\n",
        "\n",
        "        x2 = self.E_FeedFor(x)\n",
        "        enc_output = self.norm2(x + self.dropout(x2))\n",
        "        return enc_output\n",
        "\n",
        "class Multi_Decoder(nn.Module):\n",
        "    def __init__(self, d_model, heads, hidden_lay, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        #Decoder stuff\n",
        "        self.D_Mask = Masked_Attention(heads, d_model, dropout)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.Cross_att = Multihead(heads, d_model, dropout)\n",
        "        self.norm4 = nn.LayerNorm(d_model)\n",
        "        self.D_Feedfor = Position_Feedforward(d_model, hidden_lay)\n",
        "        self.norm5 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, tgt_mask=None, src_mask=None):\n",
        "        x3 = self.D_Mask(x, mask=tgt_mask)\n",
        "        x = self.norm3(x+ self.dropout(x3))\n",
        "\n",
        "\n",
        "        x4 = self.Cross_att(x, enc_output, mask=src_mask)\n",
        "        x = self.norm4(x + self.dropout(x4))\n",
        "\n",
        "        x5 = self.D_Feedfor(x)\n",
        "        x = self.norm5(x + self.dropout(x5))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class FullTransformer_Custom(nn.Module):\n",
        "    def __init__(self, vocab_size, heads, d_model, hidden_lay,seq_len, num_layers,dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.src_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.pos = Positional_Encoding(seq_len, d_model)\n",
        "        self.pos1 = Positional_Encoding(seq_len,d_model)\n",
        "\n",
        "        # Create 6 encoder and 6 decoder layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            Multi_Encoder(d_model, heads, hidden_lay, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            Multi_Decoder(d_model, heads, hidden_lay, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.out_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def encoder(self, src, src_mask=None):\n",
        "        # Doing Encoder Stuff\n",
        "        x = self.src_embedding(src) * math.sqrt(self.d_model)\n",
        "        x = self.pos(x)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, src_mask=src_mask)\n",
        "        return x\n",
        "\n",
        "    def decoder(self, tar, enc_output, tgt_mask=None, src_mask=None):\n",
        "        # Doing the Decoder Stuff\n",
        "        x = self.tgt_embedding(tar) * math.sqrt(self.d_model)\n",
        "        x = self.pos1(x)\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, enc_output, tgt_mask=tgt_mask, src_mask=src_mask)\n",
        "        out = self.out_layer(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def forward(self, src, tar, src_mask=None, tgt_mask=None):\n",
        "        encoder_output = self.encoder(src, src_mask=src_mask)\n",
        "        final = self.decoder(tar, encoder_output, tgt_mask=tgt_mask, src_mask=src_mask)\n",
        "        return final\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EI0xkiVH2qS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "cbqn1ZF338IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Willing to write the Data Loading part which gonna take care of all the\n",
        "preprocessing steps that has to be taken before feding the data to the Transformer\n",
        "'''\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "from torchtext.datasets import Multi30k\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, dataset, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import math\n",
        "from itertools import islice\n",
        "\n",
        "# Loading the Data\n",
        "train_iter, valid_iter, test_iter = Multi30k(split=('train', 'valid', 'test'), language_pair=('en', 'de'))\n",
        "\n",
        "#Initializing the Tokeniziers\n",
        "tokenizer_en = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "tokenizer_de = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
        "\n",
        "#Method to tokenizer on each word of the sentence\n",
        "def yeild_tokens(datasets, tokenizer, index = 0):\n",
        "    for data in datasets:\n",
        "        yield tokenizer(data[index])\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "# Using default build_vocab_from_iterator to find all the unique words and assigning them an integer\n",
        "#For english sentences\n",
        "eng_tokens = build_vocab_from_iterator(yeild_tokens(train_iter, tokenizer_en), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"], max_tokens=8000)\n",
        "eng_tokens.set_default_index(eng_tokens[\"<unk>\"])\n",
        "\n",
        "train_iter, valid_iter, test_iter = Multi30k(split=('train', 'valid', 'test'), language_pair=('en', 'de'))\n",
        "#For German Sentences so keeping the index as 1\n",
        "ger_tokens = build_vocab_from_iterator(yeild_tokens(train_iter, tokenizer_de, index=1), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"], max_tokens=8000)\n",
        "ger_tokens.set_default_index(ger_tokens[\"<unk>\"])\n",
        "#print(len(eng_tokens))\n",
        "#print(len(ger_tokens))\n",
        "\n",
        "#Method to apply these vocab to data\n",
        "def apply_vocab(src_text, tgr_text):\n",
        "    src_tokens = [eng_tokens[\"<bos>\"]] + [eng_tokens[t] for t in tokenizer_en(src_text)] + [eng_tokens[\"<eos>\"]]\n",
        "    tgt_tokens = [ger_tokens[\"<bos>\"]] + [ger_tokens[t] for t in tokenizer_de(tgr_text)] + [ger_tokens[\"<eos>\"]]\n",
        "\n",
        "    src_tensor = torch.tensor(src_tokens)\n",
        "    tgr_tensor = torch.tensor(tgt_tokens)\n",
        "\n",
        "    return src_tensor, tgr_tensor\n",
        "\n",
        "#Method to apply padding\n",
        "def padding(src, tar):\n",
        "    pad_src = pad_sequence(src, batch_first= True, padding_value=eng_tokens[\"<pad>\"])\n",
        "    pad_tar = pad_sequence(tar, batch_first=True, padding_value=ger_tokens[\"<pad>\"])\n",
        "\n",
        "    src_op = (pad_src == eng_tokens[\"<pad>\"])\n",
        "    tar_op = (pad_tar == ger_tokens[\"<pad>\"])\n",
        "\n",
        "    return pad_src, pad_tar, src_op, tar_op\n",
        "\n",
        "#Applying custom apply_vocab and padding function to the train data using a function called collat\n",
        "def collate_fn(batch):\n",
        "    src_list , tgr_list = [], []\n",
        "\n",
        "    for src, tgr in batch:\n",
        "        src_tensor , tgr_tensor = apply_vocab(src, tgr)\n",
        "        src_list.append(src_tensor)\n",
        "        tgr_list.append(tgr_tensor)\n",
        "\n",
        "    return padding(src_list, tgr_list)\n",
        "\n",
        "#Now we has to write a Custom Dataloader that uses this collate function\n",
        "train_iter, valid_iter, test_iter = Multi30k(split=('train', 'valid', 'test'), language_pair=('en', 'de'))\n",
        "train_loader = DataLoader(train_iter, batch_size = 32, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(valid_iter, batch_size = 32, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_iter, batch_size = 32, collate_fn=collate_fn)\n",
        "\n",
        "print(\"\\nTesting DataLoader...\")\n",
        "src, tgt, src_mask, tgt_mask = next(iter(train_loader))\n",
        "print(f\"Source shape: {src.shape}\")\n",
        "print(f\"Target shape: {tgt.shape}\")\n",
        "print(f\"Source mask shape: {src_mask.shape}\")\n",
        "print(f\"Target mask shape: {tgt_mask.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GvbfpWJ2qdl",
        "outputId": "b2bbc51e-fcce-4629-c023-c87afd87b419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Testing DataLoader...\n",
            "Source shape: torch.Size([32, 24])\n",
            "Target shape: torch.Size([32, 21])\n",
            "Source mask shape: torch.Size([32, 24])\n",
            "Target mask shape: torch.Size([32, 21])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Training the Custom Transformer model with Mixied precesion and Gradient Clipping\n",
        "'''\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "#Adding Validation Step after each loop\n",
        "vocab_size = max(len(eng_tokens), len(ger_tokens))\n",
        "model = FullTransformer_Custom(vocab_size, heads = 8, d_model=256, hidden_lay=512, seq_len=100, num_layers=3, dropout=0.2)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    ignore_index=ger_tokens[\"<pad>\"],\n",
        "    label_smoothing=0.1\n",
        ")\n",
        "optimizer = optim.AdamW(  # Changed from Adam\n",
        "    model.parameters(),\n",
        "    lr=0.0001,           # Reduced from 0.0003\n",
        "    betas=(0.9, 0.98),\n",
        "    eps=1e-9,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=3,          # Reduced from 5\n",
        "    min_lr=1e-6\n",
        ")\n",
        "epochs = 30\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for i in range(epochs):\n",
        "    print(\"Running Epoch Number\", i)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    for src, tgr, src_mask, tgr_mask in train_loader:\n",
        "        src = src.to(device)\n",
        "        tgr = tgr.to(device)\n",
        "        src_mask = src_mask.to(device)\n",
        "        tgr_mask = tgr_mask.to(device)\n",
        "\n",
        "        tgr_input = tgr[:, :-1]\n",
        "        tgr_mask_input = tgr_mask[:, :-1]\n",
        "        tgr_output = tgr[:, 1:]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #Adding Mixed Precesion to speed up the training\n",
        "        with torch.cuda.amp.autocast():\n",
        "             output = model(src, tgr_input, src_mask=src_mask, tgt_mask=tgr_mask_input)\n",
        "             loss = criterion(output.reshape(-1, vocab_size), tgr_output.reshape(-1))\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        batch_count += 1\n",
        "    avg_loss = total_loss / batch_count\n",
        "    print(f\"Epoch {i+1}/{epochs}, Loss: {avg_loss:.4f}, Perplexity: {math.exp(avg_loss):.2f}\")\n",
        "\n",
        "    #Adding Validation after each Epoch to generalize the model better\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_count = 0\n",
        "    with torch.no_grad():\n",
        "         for src, tgr, src_mask, tgr_mask in val_loader:\n",
        "             src = src.to(device)\n",
        "             tgr = tgr.to(device)\n",
        "             src_mask = src_mask.to(device)\n",
        "             tgr_mask = tgr_mask.to(device)\n",
        "             tgr_input = tgr[:, :-1]\n",
        "             tgr_output = tgr[:, 1:]\n",
        "             tgr_mask_input = tgr_mask[:, :-1]\n",
        "\n",
        "             #Adding Mixed Precesion to speed up the training\n",
        "             with torch.cuda.amp.autocast():\n",
        "                  output = model(src, tgr_input, src_mask=src_mask, tgt_mask=tgr_mask_input)\n",
        "                  loss = criterion(output.reshape(-1, vocab_size), tgr_output.reshape(-1))\n",
        "\n",
        "             val_loss += loss.item()\n",
        "             val_count += 1\n",
        "    avg_val_loss = val_loss / val_count\n",
        "    print(f\"Epoch {i+1}/{epochs}, Val Loss: {avg_val_loss:.4f}, Perplexity: {math.exp(avg_val_loss):.2f}\\n\")\n",
        "    scheduler.step(avg_val_loss)\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save({\n",
        "            'epoch': i + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_loss': avg_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "        }, 'best_transformer_model.pth')\n",
        "        print(f\"âœ“ Best model saved! Val Loss: {avg_val_loss:.4f}\\n\")\n",
        "\n",
        "\n",
        "#Saving the Model\n",
        "# Save the model after training completes\n",
        "torch.save({\n",
        "    'epoch': epochs,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'train_loss': avg_loss,\n",
        "    'val_loss': avg_val_loss,\n",
        "}, 'final_transformer_model.pth')\n",
        "print(f\"Training complete!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Final model saved as 'final_transformer_model.pth'\")\n",
        "print(f\"Best model saved as 'best_transformer_model.pth'\")\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading models to your computer...\")\n",
        "files.download('best_transformer_model.pth')\n",
        "files.download('final_transformer_model.pth')\n",
        "print(\"Download complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r7CCLfe4-XhK",
        "outputId": "d29466f5-c266-4487-9cac-e8b69f8ba430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Epoch Number 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 5.1561, Perplexity: 173.49\n",
            "Epoch 1/30, Val Loss: 4.4798, Perplexity: 88.22\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 4.4798\n",
            "\n",
            "Running Epoch Number 1\n",
            "Epoch 2/30, Loss: 4.3313, Perplexity: 76.04\n",
            "Epoch 2/30, Val Loss: 4.0865, Perplexity: 59.53\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 4.0865\n",
            "\n",
            "Running Epoch Number 2\n",
            "Epoch 3/30, Loss: 4.0311, Perplexity: 56.32\n",
            "Epoch 3/30, Val Loss: 3.8636, Perplexity: 47.64\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.8636\n",
            "\n",
            "Running Epoch Number 3\n",
            "Epoch 4/30, Loss: 3.8284, Perplexity: 45.99\n",
            "Epoch 4/30, Val Loss: 3.7103, Perplexity: 40.86\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.7103\n",
            "\n",
            "Running Epoch Number 4\n",
            "Epoch 5/30, Loss: 3.6751, Perplexity: 39.45\n",
            "Epoch 5/30, Val Loss: 3.5921, Perplexity: 36.31\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.5921\n",
            "\n",
            "Running Epoch Number 5\n",
            "Epoch 6/30, Loss: 3.5524, Perplexity: 34.90\n",
            "Epoch 6/30, Val Loss: 3.5122, Perplexity: 33.52\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.5122\n",
            "\n",
            "Running Epoch Number 6\n",
            "Epoch 7/30, Loss: 3.4498, Perplexity: 31.49\n",
            "Epoch 7/30, Val Loss: 3.4263, Perplexity: 30.76\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.4263\n",
            "\n",
            "Running Epoch Number 7\n",
            "Epoch 8/30, Loss: 3.3612, Perplexity: 28.82\n",
            "Epoch 8/30, Val Loss: 3.3709, Perplexity: 29.10\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.3709\n",
            "\n",
            "Running Epoch Number 8\n",
            "Epoch 9/30, Loss: 3.2811, Perplexity: 26.60\n",
            "Epoch 9/30, Val Loss: 3.3174, Perplexity: 27.59\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.3174\n",
            "\n",
            "Running Epoch Number 9\n",
            "Epoch 10/30, Loss: 3.2130, Perplexity: 24.85\n",
            "Epoch 10/30, Val Loss: 3.2608, Perplexity: 26.07\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.2608\n",
            "\n",
            "Running Epoch Number 10\n",
            "Epoch 11/30, Loss: 3.1491, Perplexity: 23.31\n",
            "Epoch 11/30, Val Loss: 3.2314, Perplexity: 25.32\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.2314\n",
            "\n",
            "Running Epoch Number 11\n",
            "Epoch 12/30, Loss: 3.0931, Perplexity: 22.04\n",
            "Epoch 12/30, Val Loss: 3.1936, Perplexity: 24.38\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.1936\n",
            "\n",
            "Running Epoch Number 12\n",
            "Epoch 13/30, Loss: 3.0398, Perplexity: 20.90\n",
            "Epoch 13/30, Val Loss: 3.1577, Perplexity: 23.52\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.1577\n",
            "\n",
            "Running Epoch Number 13\n",
            "Epoch 14/30, Loss: 2.9922, Perplexity: 19.93\n",
            "Epoch 14/30, Val Loss: 3.1243, Perplexity: 22.74\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.1243\n",
            "\n",
            "Running Epoch Number 14\n",
            "Epoch 15/30, Loss: 2.9470, Perplexity: 19.05\n",
            "Epoch 15/30, Val Loss: 3.0944, Perplexity: 22.07\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.0944\n",
            "\n",
            "Running Epoch Number 15\n",
            "Epoch 16/30, Loss: 2.9064, Perplexity: 18.29\n",
            "Epoch 16/30, Val Loss: 3.0877, Perplexity: 21.93\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.0877\n",
            "\n",
            "Running Epoch Number 16\n",
            "Epoch 17/30, Loss: 2.8690, Perplexity: 17.62\n",
            "Epoch 17/30, Val Loss: 3.0603, Perplexity: 21.33\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.0603\n",
            "\n",
            "Running Epoch Number 17\n",
            "Epoch 18/30, Loss: 2.8324, Perplexity: 16.99\n",
            "Epoch 18/30, Val Loss: 3.0427, Perplexity: 20.96\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.0427\n",
            "\n",
            "Running Epoch Number 18\n",
            "Epoch 19/30, Loss: 2.8003, Perplexity: 16.45\n",
            "Epoch 19/30, Val Loss: 3.0274, Perplexity: 20.64\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.0274\n",
            "\n",
            "Running Epoch Number 19\n",
            "Epoch 20/30, Loss: 2.7664, Perplexity: 15.90\n",
            "Epoch 20/30, Val Loss: 3.0239, Perplexity: 20.57\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.0239\n",
            "\n",
            "Running Epoch Number 20\n",
            "Epoch 21/30, Loss: 2.7373, Perplexity: 15.45\n",
            "Epoch 21/30, Val Loss: 3.0101, Perplexity: 20.29\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 3.0101\n",
            "\n",
            "Running Epoch Number 21\n",
            "Epoch 22/30, Loss: 2.7091, Perplexity: 15.02\n",
            "Epoch 22/30, Val Loss: 2.9992, Perplexity: 20.07\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 2.9992\n",
            "\n",
            "Running Epoch Number 22\n",
            "Epoch 23/30, Loss: 2.6825, Perplexity: 14.62\n",
            "Epoch 23/30, Val Loss: 2.9939, Perplexity: 19.96\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 2.9939\n",
            "\n",
            "Running Epoch Number 23\n",
            "Epoch 24/30, Loss: 2.6576, Perplexity: 14.26\n",
            "Epoch 24/30, Val Loss: 2.9813, Perplexity: 19.71\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 2.9813\n",
            "\n",
            "Running Epoch Number 24\n",
            "Epoch 25/30, Loss: 2.6329, Perplexity: 13.91\n",
            "Epoch 25/30, Val Loss: 2.9746, Perplexity: 19.58\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 2.9746\n",
            "\n",
            "Running Epoch Number 25\n",
            "Epoch 26/30, Loss: 2.6095, Perplexity: 13.59\n",
            "Epoch 26/30, Val Loss: 2.9840, Perplexity: 19.77\n",
            "\n",
            "Running Epoch Number 26\n",
            "Epoch 27/30, Loss: 2.5878, Perplexity: 13.30\n",
            "Epoch 27/30, Val Loss: 2.9702, Perplexity: 19.50\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 2.9702\n",
            "\n",
            "Running Epoch Number 27\n",
            "Epoch 28/30, Loss: 2.5672, Perplexity: 13.03\n",
            "Epoch 28/30, Val Loss: 2.9711, Perplexity: 19.51\n",
            "\n",
            "Running Epoch Number 28\n",
            "Epoch 29/30, Loss: 2.5462, Perplexity: 12.76\n",
            "Epoch 29/30, Val Loss: 2.9589, Perplexity: 19.28\n",
            "\n",
            "âœ“ Best model saved! Val Loss: 2.9589\n",
            "\n",
            "Running Epoch Number 29\n",
            "Epoch 30/30, Loss: 2.5258, Perplexity: 12.50\n",
            "Epoch 30/30, Val Loss: 2.9718, Perplexity: 19.53\n",
            "\n",
            "Training complete!\n",
            "Best validation loss: 2.9589\n",
            "Final model saved as 'final_transformer_model.pth'\n",
            "Best model saved as 'best_transformer_model.pth'\n",
            "Downloading models to your computer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5d1330e3-2750-4f68-85f0-a2d0b21ecd2f\", \"best_transformer_model.pth\", 121642839)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_be0eca6b-cc89-417b-a455-19fb98e9895f\", \"final_transformer_model.pth\", 121643368)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTING THE TRANSFORMER ON TEST DATA\n",
        "\n",
        "import urllib.request\n",
        "import gzip\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Download and prepare test data\n",
        "def download_multi30k_test():\n",
        "    \"\"\"Download Multi30k test data, bypassing corrupted cache\"\"\"\n",
        "    data_dir = Path('/tmp/multi30k_manual')\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    base_url = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "    files = {'en': 'test_2016_flickr.en.gz', 'de': 'test_2016_flickr.de.gz'}\n",
        "\n",
        "    data = {}\n",
        "    for lang, filename in files.items():\n",
        "        txt_path = data_dir / f'test_{lang}.txt'\n",
        "\n",
        "        if not txt_path.exists():\n",
        "            print(f\"Downloading {lang} test data...\")\n",
        "            urllib.request.urlretrieve(base_url + filename, data_dir / filename)\n",
        "            with gzip.open(data_dir / filename, 'rb') as f_in, open(txt_path, 'wb') as f_out:\n",
        "                f_out.write(f_in.read())\n",
        "            (data_dir / filename).unlink()\n",
        "\n",
        "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "            data[lang] = [line.strip() for line in f]\n",
        "\n",
        "    return data['en'], data['de']\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, src, tgt):\n",
        "        self.data = list(zip(src, tgt))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Download test data\n",
        "print(\"Downloading test data...\")\n",
        "test_en, test_de = download_multi30k_test()\n",
        "print(f\"Test set size: {len(test_en)} examples\\n\")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    SimpleDataset(test_en, test_de),\n",
        "    batch_size=8,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Recreate model with the EXACT architecture that was saved\n",
        "print(\"Recreating model to match checkpoint...\")\n",
        "vocab_size = max(len(eng_tokens), len(ger_tokens))\n",
        "model = FullTransformer_Custom(\n",
        "    vocab_size,\n",
        "    heads=8,\n",
        "    d_model=256,\n",
        "    hidden_lay=512,  # Your checkpoint was trained with 1024, NOT 2048\n",
        "    seq_len=100,\n",
        "    num_layers=3\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Load the best model weights\n",
        "print(\"Loading best model weights...\")\n",
        "checkpoint = torch.load('best_transformer_model.pth', map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "print(f\"âœ“ Model loaded from epoch {checkpoint['epoch']}\")\n",
        "print(f\"âœ“ Best Val Loss: {checkpoint['val_loss']:.4f}\\n\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "batch_count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, tgt, src_mask, tgt_mask in test_loader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "        src_mask = src_mask.to(device)\n",
        "        tgt_mask = tgt_mask.to(device)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "        tgt_mask_input = tgt_mask[:, :-1]\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = model(src, tgt_input, src_mask=src_mask, tgt_mask=tgt_mask_input)\n",
        "            loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "avg_test_loss = test_loss / batch_count\n",
        "test_perplexity = math.exp(avg_test_loss)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Test Loss:       {avg_test_loss:.4f}\")\n",
        "print(f\"Test Perplexity: {test_perplexity:.2f}\")\n",
        "print(f\"Batches:         {batch_count}\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7EGMjatIuJs",
        "outputId": "129d36b5-b7e1-4847-fdb8-bd6b8d9601d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading test data...\n",
            "Test set size: 1000 examples\n",
            "\n",
            "Recreating model to match checkpoint...\n",
            "Loading best model weights...\n",
            "âœ“ Model loaded from epoch 29\n",
            "âœ“ Best Val Loss: 2.9589\n",
            "\n",
            "Evaluating on test set...\n",
            "\n",
            "==================================================\n",
            "TEST RESULTS\n",
            "==================================================\n",
            "Test Loss:       2.9718\n",
            "Test Perplexity: 19.53\n",
            "Batches:         125\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence_greedy(sentence, model, eng_tokens, ger_tokens, device, max_len=100):\n",
        "    \"\"\"Simple greedy decoding with CORRECT mask format\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get vocab mappings\n",
        "    eng_stoi = eng_tokens.get_stoi()\n",
        "    ger_stoi = ger_tokens.get_stoi()\n",
        "    ger_itos = ger_tokens.get_itos()\n",
        "\n",
        "    # Tokenize source\n",
        "    tokens = tokenizer_en(sentence.lower())\n",
        "    src_indices = [eng_stoi[\"<bos>\"]] + [eng_stoi.get(t, eng_stoi[\"<unk>\"]) for t in tokens] + [eng_stoi[\"<eos>\"]]\n",
        "\n",
        "    # Pad source\n",
        "    src_len = len(src_indices)\n",
        "    if len(src_indices) < max_len:\n",
        "        src_indices += [eng_stoi[\"<pad>\"]] * (max_len - len(src_indices))\n",
        "    else:\n",
        "        src_indices = src_indices[:max_len]\n",
        "\n",
        "    src = torch.tensor([src_indices]).to(device)\n",
        "\n",
        "    # FIXED: Source mask - True for PAD tokens (to mask them out)\n",
        "    src_mask = (src == eng_stoi[\"<pad>\"])  # Shape: [batch, seq_len]\n",
        "\n",
        "    # Start with <bos>\n",
        "    tgt_indices = [ger_stoi[\"<bos>\"]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len - 1):\n",
        "            seq_len = len(tgt_indices)\n",
        "\n",
        "            # Pad current target\n",
        "            tgt_padded = tgt_indices + [ger_stoi[\"<pad>\"]] * (max_len - seq_len)\n",
        "            tgt = torch.tensor([tgt_padded]).to(device)\n",
        "\n",
        "            # FIXED: Target mask - True for PAD tokens\n",
        "            tgt_mask = (tgt == ger_stoi[\"<pad>\"])  # Shape: [batch, seq_len]\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "            # Get next token from the LAST GENERATED position\n",
        "            next_token = output[0, seq_len - 1].argmax().item()\n",
        "\n",
        "            if next_token == ger_stoi[\"<eos>\"]:\n",
        "                break\n",
        "\n",
        "            tgt_indices.append(next_token)\n",
        "\n",
        "    # Convert to words\n",
        "    translated_tokens = []\n",
        "    for idx in tgt_indices[1:]:  # Skip <bos>\n",
        "        if idx == ger_stoi[\"<eos>\"]:\n",
        "            break\n",
        "        token = ger_itos[idx]\n",
        "        if token not in [\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"]:\n",
        "            translated_tokens.append(token)\n",
        "\n",
        "    return ' '.join(translated_tokens) if translated_tokens else \"<empty>\"\n",
        "\n",
        "\n",
        "def translate_sentence_beam(sentence, model, eng_tokens, ger_tokens, device, max_len=100, beam_width=5):\n",
        "    \"\"\"Translate using beam search with CORRECT mask format\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize source\n",
        "    tokens = tokenizer_en(sentence.lower())\n",
        "\n",
        "    # Get vocab mappings\n",
        "    eng_stoi = eng_tokens.get_stoi()\n",
        "    ger_stoi = ger_tokens.get_stoi()\n",
        "\n",
        "    src_indices = [eng_stoi[\"<bos>\"]] + [eng_stoi.get(t, eng_stoi[\"<unk>\"]) for t in tokens] + [eng_stoi[\"<eos>\"]]\n",
        "\n",
        "    # Pad source to max_len\n",
        "    if len(src_indices) < max_len:\n",
        "        src_indices += [eng_stoi[\"<pad>\"]] * (max_len - len(src_indices))\n",
        "    else:\n",
        "        src_indices = src_indices[:max_len]\n",
        "\n",
        "    src = torch.tensor([src_indices]).to(device)\n",
        "\n",
        "    # FIXED: Source mask - True for PAD tokens\n",
        "    src_mask = (src == eng_stoi[\"<pad>\"])\n",
        "\n",
        "    # Initialize beam\n",
        "    beams = [([ger_stoi[\"<bos>\"]], 0.0)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step in range(max_len - 1):\n",
        "            all_candidates = []\n",
        "\n",
        "            for seq, score in beams:\n",
        "                if seq[-1] == ger_stoi[\"<eos>\"]:\n",
        "                    all_candidates.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                seq_len = len(seq)\n",
        "\n",
        "                # Pad target to max_len\n",
        "                tgt_padded = seq + [ger_stoi[\"<pad>\"]] * (max_len - seq_len)\n",
        "                tgt = torch.tensor([tgt_padded]).to(device)\n",
        "\n",
        "                # FIXED: Target mask - True for PAD tokens\n",
        "                tgt_mask = (tgt == ger_stoi[\"<pad>\"])\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    output = model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "                # Get logits for last valid position\n",
        "                logits = output[0, seq_len - 1]\n",
        "                log_probs = torch.log_softmax(logits, dim=-1)\n",
        "                top_probs, top_indices = torch.topk(log_probs, beam_width)\n",
        "\n",
        "                for prob, idx in zip(top_probs, top_indices):\n",
        "                    new_seq = seq + [idx.item()]\n",
        "                    new_score = score + prob.item()\n",
        "                    all_candidates.append((new_seq, new_score))\n",
        "\n",
        "            # Keep top beam_width sequences\n",
        "            beams = sorted(all_candidates, key=lambda x: x[1] / len(x[0]), reverse=True)[:beam_width]\n",
        "\n",
        "            if all(seq[-1] == ger_stoi[\"<eos>\"] for seq, _ in beams):\n",
        "                break\n",
        "\n",
        "    # Get best sequence\n",
        "    best_seq = beams[0][0]\n",
        "    ger_itos = ger_tokens.get_itos()\n",
        "\n",
        "    # Convert to words\n",
        "    translated_tokens = []\n",
        "    for idx in best_seq[1:]:\n",
        "        if idx == ger_stoi[\"<eos>\"]:\n",
        "            break\n",
        "        token = ger_itos[idx]\n",
        "        if token not in [\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"]:\n",
        "            translated_tokens.append(token)\n",
        "\n",
        "    return ' '.join(translated_tokens) if translated_tokens else \"<empty>\"\n",
        "\n",
        "\n",
        "# Test\n",
        "test_sentences = [\n",
        "    \"a dog is running in the park\",\n",
        "    \"the cat is sleeping on the bed\",\n",
        "    \"two people are walking together\",\n",
        "    \"children are playing with a ball\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GREEDY DECODING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    translation = translate_sentence_greedy(sentence, model, eng_tokens, ger_tokens, device)\n",
        "    print(f\"\\nEnglish:  {sentence}\")\n",
        "    print(f\"German:   {translation}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BEAM SEARCH (width=5)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    translation = translate_sentence_beam(sentence, model, eng_tokens, ger_tokens, device, beam_width=5)\n",
        "    print(f\"\\nEnglish:  {sentence}\")\n",
        "    print(f\"German:   {translation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzdXcPhASfHm",
        "outputId": "0239384b-123f-4432-f6c4-080da2c8098f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GREEDY DECODING\n",
            "============================================================\n",
            "\n",
            "English:  a dog is running in the park\n",
            "German:   Ein Hund lÃ¤uft im Park .\n",
            "\n",
            "English:  the cat is sleeping on the bed\n",
            "German:   Eine Katze schlÃ¤ft auf dem Bett .\n",
            "\n",
            "English:  two people are walking together\n",
            "German:   Zwei Personen gehen zusammen spazieren .\n",
            "\n",
            "English:  children are playing with a ball\n",
            "German:   Kinder spielen mit einem Ball .\n",
            "\n",
            "============================================================\n",
            "BEAM SEARCH (width=5)\n",
            "============================================================\n",
            "\n",
            "English:  a dog is running in the park\n",
            "German:   Ein Hund lÃ¤uft in einem Park .\n",
            "\n",
            "English:  the cat is sleeping on the bed\n",
            "German:   Eine Katze schlÃ¤ft auf dem Bett .\n",
            "\n",
            "English:  two people are walking together\n",
            "German:   Zwei Personen gehen zusammen spazieren .\n",
            "\n",
            "English:  children are playing with a ball\n",
            "German:   Kinder spielen mit einem Ball .\n"
          ]
        }
      ]
    }
  ]
}